# [triton language](https://triton-lang.org/main/python-api/triton.language.html)

ç†è§£tritonè¯­æ³•çš„repoï¼š[triton-puzzles](https://github.com/srush/Triton-Puzzles)

[ğŸŒ°: vector-add](./Triton_base.md##elements)

```python
import torch
import triton
import triton.language as tl
```

## detector

### triton.autotune

è‡ªåŠ¨è°ƒä¼˜detectorï¼Œç”¨äºè‡ªåŠ¨æ‰¾åˆ°æœ€ä½³é…ç½®

ä½¿ç”¨ä¸Šéœ€è¦æä¾›ä¸€ä¸ªconfigsï¼ˆåŒ…å«åœ¨kernelä¸­å®šä¹‰çš„ `tl.constexpr`ï¼‰åˆ—è¡¨ï¼Œautotuneä¼šå¤šæ¬¡è¿è¡Œkernelå‡½æ•°æ¥è¯„ä¼°configsä¸­çš„æ‰€æœ‰é…ç½®ã€‚ï¼ˆé…ç½®æ˜¯äººä¸ºç»™å‡ºçš„ï¼Œæ‰€ä»¥ç©ºé—´ä¸å¤§ï¼Œä¾èµ–äººä¸ºç»éªŒï¼‰

- keyï¼šå‚æ•°åˆ—è¡¨ï¼Œå½“keyä¸­çš„å‚æ•°æ”¹å˜æ—¶ï¼Œéœ€è¦é‡æ–°è¯„ä¼°configs

- prune_configs_byï¼šç”¨æˆ·å¯ä»¥ä¼ å…¥å‡½æ•°æ¥å¸®åŠ©å‡æï¼ˆä¾‹å¦‚åŸºäºæ€§èƒ½æ¨¡å‹çš„å‡½æ•°ï¼‰ï¼ŒåŠ å¿«æ”¶æ•›

- reset_to_zeroï¼šè¾“å…¥å‚æ•°ååˆ—è¡¨ï¼Œåœ¨è¿è¡Œå‰å°†è¿™äº›å‚æ•°é‡ç½®ä¸º0

- warmupï¼šæ¯ä¸ªconfigçš„warmupæ—¶é—´ï¼Œé»˜è®¤25ms

- repï¼šæ¯ä¸ªconfigçš„é‡å¤æ—¶é—´ï¼Œé»˜è®¤100ns


```python
def cfggen():
    configs=[
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
    ],
    return configs

def prune_config(configs, named_args, **kwargs):
    M = named_args["M"]
    N = named_args["N"]
    K = named_args["K"]
    pruned_configs = []
    for config in configs:
        BLOCK_M = config.kwargs["BLOCK_SIZE_M"]
        BLOCK_N = config.kwargs["BLOCK_SIZE_N"]
        BLOCK_K = config.kwargs["BLOCK_SIZE_K"]
        # Drop useless configs.
        if ((M // BLOCK_M < 1) || (N // BLOCK_N < 1) || (K // BLOCK_K < 1)):
            continue
        if (......):
            continue
        pruned_configs.append(config)
    # Only keep 4 configs.
    if len(pruned_configs) > 4:
        pruned_configs = pruned_configs[-4:]
    return pruned_configs


@triton.autotune(
      configs=cfggen(),
      prune_configs_by={'early_config_prune': prune_config},
      key=["M", "N", "N"]
)
```

### triton.Config

triton.Config(self, kwargs, num_warps=4, num_stages=2, num_ctas=1, pre_hook=None)

- kwargs: è¶…å‚æ•°çš„å­—å…¸

- num_warps: ç¨‹åºæ‰§è¡Œä½¿ç”¨çš„warpæ•°é‡

- num_stages: è½¯ä»¶æµæ°´çº¿çš„å‚æ•°

- num_ctas: ä¸€ä¸ªblock clusterä¸­çš„blockæ•°é‡

```python
triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2)
```

### triton.heuristics

å¯å‘å¼detectorï¼Œæ ¹æ®è¾“å…¥å‚æ•°åŠ¨æ€è°ƒæ•´ kernel çš„è¡Œä¸º

ä¾‹å¦‚ï¼Œå¦‚æœBï¼ˆåç½®ï¼‰ä¸ä¸ºNoneï¼Œåˆ™HAS_BIASä¸ºçœŸ

```python
@triton.heuristics({"HAS_X1": lambda args: args["X1"] is not None})
@triton.heuristics({"HAS_W1": lambda args: args["W1"] is not None})
@triton.heuristics({"HAS_B1": lambda args: args["B1"] is not None})
```

## tl.constexpr

è¶…å‚æ•°ï¼Œå¯¹äºä¸åŒçš„ç¡¬ä»¶ä½¿ç”¨æ—¶ï¼Œæœ€ä½³æ€§èƒ½çš„å‚æ•°å¯èƒ½æ˜¯ä¸åŒçš„ï¼Œå…¶å€¼ç”± Triton Compiler è¿›è¡Œæœç´¢ï¼Œä¼šäººä¸ºç»™ä¸€ä¸ªç”± **`@auto-tuning` æ ‡è®°çš„ `configs`**ï¼ˆä¾èµ–äººä¸ºç»éªŒï¼‰ã€‚

## Memory/Pointer Ops

```bash
load
store
make_block_ptr
advance
```


### tl.load & tl.store

load(*pointer*, *mask=None*, *other=None*, *boundary_check=()*, *padding_option=''*, *cache_modifier=''*, *eviction_policy=''*, *volatile=False*).

ä¸»è¦æ˜¯å‰ä¸‰ä¸ªå‚æ•°ï¼š

- pointer: è¾“å…¥çš„æŒ‡é’ˆï¼Œç›´æ¥ä¼ å…¥å¯¹è±¡æ•°ç»„å³å¯
- mask: ä¿è¯readå’Œwriteæ—¶çš„è¾¹ç•Œï¼Œåªè¯»è¿™éƒ¨åˆ†
- other: loadåï¼Œå…¶ä»–åŒºåŸŸçš„å¡«å……å€¼

```python
@triton.jit
def demo(x_ptr):
    i_range = tl.arange(0, 8)[:, None]
    j_range = tl.arange(0, 4)[None, :]
    range = i_range * 4 + j_range # è¿™ä¸ª4æ˜¯æ­¥é•¿ï¼Œä»£è¡¨ç¬¬ä¸€ç»´æ¯ä¸ªæ•°æ®ä¹‹é—´å·®4
    print(range)
    x = tl.load(x_ptr + range, (i_range < 4) & (j_range < 3), 0)
    print(x)

demo(torch.ones(4, 4))
```

> å¦‚æœè¾“å…¥æ˜¯å¤šç»´çš„ï¼Œåœ¨ç»™rangeæ—¶ä¸€å®šè¦ç»™æ­£ç¡®çš„strideï¼Œä¸ç„¶ä¼šé‡å¤è¯»

è¾“å‡º

```python
range = [[ 0  1  2  3] x = [[1. 1. 1. 0.]
         [ 4  5  6  7]      [1. 1. 1. 0.]
         [ 8  9 10 11]      [1. 1. 1. 0.]
         [12 13 14 15]      [1. 1. 1. 0.]
         [16 17 18 19]      [0. 0. 0. 0.]
         [20 21 22 23]      [0. 0. 0. 0.]
         [24 25 26 27]      [0. 0. 0. 0.]
         [28 29 30 31]]     [0. 0. 0. 0.]]
```

<div style="text-align: center;"><img src="./img_Triton_language/load.png" alt="load" style="width: 60%;"></div>


store(*pointer*, *value*, *mask=None*, *boundary_check=()*, *cache_modifier=''*, *eviction_policy=''*)

ä¸»è¦æ˜¯å‰ä¸‰ä¸ªå‚æ•°ï¼š

- pointer: è¾“å…¥çš„æŒ‡é’ˆï¼Œç›´æ¥ä¼ å…¥å¯¹è±¡æ•°ç»„å³å¯
- value: å†™å…¥çš„å€¼
- mask:  ä¿è¯readå’Œwriteæ—¶çš„è¾¹ç•Œï¼Œåªæœ‰è¿™éƒ¨åˆ†ä¼šè¢«å†™ï¼Œå…¶ä»–ä¿æŒåŸå€¼

```python
@triton.jit
def demo(z_ptr):
    range = tl.arange(0, 8)
    z = tl.store(z_ptr + range, 10, range < 5)
    print(z)

demo(z)
```

è¾“å‡ºï¼Œåªæœ‰maskå†…çš„rangeè¢«å†™äº†value

```python
tensor([[10., 10., 10.],
        [10., 10.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])
```

<div style="text-align: center;"><img src="./img_Triton_language/store.png" alt="store" style="width: 70%;"></div>


**ä¸åŒpidçš„loadæ˜¯å¹¶è¡Œçš„**

```python
@triton.jit
def demo(x_ptr):
    pid = tl.program_id(0)
    range = tl.arange(0, 8) + pid * 8 # pid = 0, 1, 2
    x = tl.load(x_ptr + range, range < 20)
    print("Print for each", pid, x)

demo(torch.ones(2, 4, 4))
```

ä¸‹å›¾æ˜¯pid=0 / 1/ 2 æ—¶æ‰€loadçš„æ•°æ®

<div style="display: flex;">
  <img src="./img_Triton_language/loadpid0.png" style="flex: 1;width: 30%;">
  <img src="./img_Triton_language/loadpid1.png" style="flex: 1;width: 30%;">
  <img src="./img_Triton_language/loadpid2.png" style="flex: 1;width: 30%;">
</div>

**cache_modifier**

load å’Œ store éƒ½æœ‰cache optionï¼Œå¯ä»¥ç”¨æ¥æ§åˆ¶cacheçš„è¡Œä¸º

ä¾‹å¦‚ `a = tl.load(a_ptrs, cache_modifier=".cg")` è¡¨ç¤ºä½¿ç”¨ LLC è¿›è¡Œè®¿å­˜ã€‚

> LLC(Last Level Cache)æ˜¯èŠ¯ç‰‡ä¸­çš„æœ€åä¸€çº§ç¼“å­˜ï¼Œä¸€èˆ¬ä¹Ÿç§°ä¸º L2_Cacheï¼Œæ•´ä¸ªèŠ¯ç‰‡å…±äº«ï¼Œå¯ä»¥è·å¾—æ›´å¤§çš„è®¿å­˜å¸¦å®½å’Œæ›´ä½çš„è®¿å­˜å»¶è¿Ÿã€‚ä¸€èˆ¬å¯¹å°è§„æ¨¡kernelæ•ˆæœæ˜¾è‘—ã€‚

```python
def _str_to_load_cache_modifier(cache_modifier):
    cache = ir.CACHE_MODIFIER.NONE  # default
    if cache_modifier:
        if cache_modifier == ".ca":
            cache = ir.CACHE_MODIFIER.CA
        elif cache_modifier == ".cg":
            cache = ir.CACHE_MODIFIER.CG
        else:
            raise ValueError(f"Cache modifier {cache_modifier} not supported")
    return cache


def _str_to_store_cache_modifier(cache_modifier):
    cache = ir.CACHE_MODIFIER.NONE  # default
    if cache_modifier:
        if cache_modifier == ".wb":
            cache = ir.CACHE_MODIFIER.WB
        elif cache_modifier == ".cg":
            cache = ir.CACHE_MODIFIER.CG
        elif cache_modifier == ".cs":
            cache = ir.CACHE_MODIFIER.CS
        elif cache_modifier == ".wt":
            cache = ir.CACHE_MODIFIER.WT
        else:
            raise ValueError(f"Cache modifier {cache_modifier} not supported")
    return cache
```


## Programming Model

```bash
    tensor
    program_id
    num_programs
```

### program_id

Input: axis

axis (int) â€“ The axis of the 3D launch grid. Must be 0, 1 or 2.

è¿”å›å½“å‰ç¨‹åºåœ¨axisä¸Šçš„id

### num_programs

Input: axis

è¿”å›axisä¸Šå½“å‰æœ‰å¤šå°‘ç¨‹åºå®ä¾‹

## Creation Ops

```bash
    arange
    cat
    full
    zeros
    zeros_like
```

## Shape Manipulation Ops

```bash
    broadcast
    broadcast_to
    expand_dims
    interleave
    join
    permute
    ravel
    reshape
    split
    trans
    view
```

## Linear Algebra Ops

```bash
    dot
```

## Indexing Ops

```bash
    flip
    where
    swizzle2d
```

## Math Ops

```bash
    abs
    cdiv
    clamp
    cos
    div_rn
    erf
    exp
    exp2
    fma
    fdiv
    floor
    log
    log2
    maximum
    minimum
    sigmoid
    sin
    softmax
    sqrt
    sqrt_rn
    umulhi
```

## Reduction Ops
```bash
    argmax
    argmin
    max
    min
    reduce
    sum
    xor_sum
```

## Scan/Sort Ops
```bash
    associative_scan
    cumprod
    cumsum
    histogram
    sort
```

## Atomic Ops
```bash
    atomic_add
    atomic_and
    atomic_cas
    atomic_max
    atomic_min
    atomic_or
    atomic_xchg
    atomic_xor
```

## Random Number Generation
```bash
    randint4x
    randint
    rand
    randn
```

## Iterators
```bash
    range
    static_range
```

## Inline Assembly
```bash
    inline_asm_elementwise
```

## Compiler Hint Ops

```bash
    debug_barrier
    max_constancy
    max_contiguous
    multiple_of
```

### debug_barrier

æ’å…¥ä¸€ä¸ªbarrieræ¥åŒæ­¥blockä¸­çš„æ‰€æœ‰thread

### tl.max_contiguous & tl.max_constany & tl.multiple_of

```python
  offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
  offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
```

ç”±äºç¼–è¯‘å™¨æ— æ³•æ„ŸçŸ¥æ•°æ®çš„è¿ç»­æ€§ï¼Œæ‰€ä»¥åŠ è½½æ•°æ®æ—¶ä¼š**ç¦»æ•£åœ°**å¤„ç†æ•°æ®ã€‚
å¦‚æœç¼–å†™kernelæ—¶æå‰å·²çŸ¥æ•°æ®è¿ç»­ï¼Œå¯ä»¥ä½¿ç”¨ `tl.max_contiguous & tl.multiple_of` å»æ ‡è¯†åŠ è½½æ•°æ®çš„è¿ç»­æ€§ï¼Œè¿™æ ·ç¼–è¯‘å™¨å°±å¯è¿ç»­åœ°å¤„ç†è¯¥æ®µæ•°æ®ã€‚

input å’Œ values æ˜¯ç­‰ç»´åº¦çš„

- max_contiguous(input, values)ï¼šå¯¹äºæ¯ä¸ªç»´åº¦iï¼Œæ ‡è¯†input[i]ä¸­ æ¯values[i]ä¸ªç›¸é‚»å…ƒç´  æ˜¯è¿ç»­çš„

> ä¾‹å¦‚ values = [4], åˆ™ input å¯ä»¥æ˜¯ [0, 1, 2, 3, 8, 9, 10, 11]

- max_constany(input, values)ï¼šå¯¹äºæ¯ä¸ªç»´åº¦iï¼Œæ ‡è¯†input[i]ä¸­ æ¯values[i]ä¸ªç›¸é‚»å…ƒç´  æ˜¯å¸¸æ•°

> ä¾‹å¦‚ values = [4], åˆ™ input å¯ä»¥æ˜¯ [0, 0, 0, 0, 1, 1, 1, 1]

- multiple_of(input, values)ï¼šå¯¹äºæ¯ä¸ªç»´åº¦iï¼Œæ ‡è¯†input[i]ä¸­ æ‰€æœ‰å…ƒç´ éƒ½æ˜¯ values[i] çš„å€æ•°

> ä¾‹å¦‚ values = [2], åˆ™ input å¯ä»¥æ˜¯ [0, 2, 4, 6, 8]

```python
  offs_am = tl.max_contiguous(tl.multiple_of((pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M, BLOCK_SIZE_M), BLOCK_SIZE_M)
  offs_am = tl.max_contiguous(tl.multiple_of((pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M, BLOCK_SIZE_M), BLOCK_SIZE_M)
```


## Debug Ops
```bash
    static_print
    static_assert
    device_print
    device_assert
```

pdb

```python
import pdb
pdb.set_trace()
```

### tl.device_print

å½“å‘ç°opç²¾åº¦æµ‹è¯•å¤±è´¥,å¯ä»¥å›ºå®šæµ‹è¯•è§„æ¨¡å’Œ `tuning config`(ä¿è¯åªæœ‰ä¸€ä¸ªï¼Œä¸ç„¶ä¼šå¤šæ¬¡print) ,ç„¶åä½¿ç”¨ `print` å¤§æ³•ã€‚

ä¾‹å¦‚å®šä½ `layernorm backward` ç²¾åº¦æ—¶ï¼Œä¾‹å¦‚
```python
if pid == 1:
tl.device_print("off:", off)
tl.device_print("offset:", offset)
tl.device_print("mask:", mask)
tl.device_print("x: ", x)
tl.device_print("dy: ", dy)
tl.device_print("w: ", w)
tl.device_print("mean: ", mean)
tl.device_print("r: ", rstd)
tl.device_print("dx: ", dx)
```