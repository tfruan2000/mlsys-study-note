# triton language

## ğŸŒ°ï¼šadd

```python
import torch

import triton
import triton.language as tl

@triton.jit
def add_kernel(x_ptr,  # *Pointer* to first input vector.
               y_ptr,  # *Pointer* to second input vector.
               output_ptr,  # *Pointer* to output vector.
               n_elements,  # Size of the vector.
               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
               # NOTE: `constexpr` so it can be used as a shape value.
               ):
    # There are multiple 'programs' processing different data. We identify which program
    # we are here:
    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.
    # This program will process inputs that are offset from the initial data.
    # For instance, if you had a vector of length 256 and block_size of 64, the programs
    # would each access the elements [0:64, 64:128, 128:192, 192:256].
    # Note that offsets is a list of pointers:
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    # Create a mask to guard memory operations against out-of-bounds accesses.
    mask = offsets < n_elements
    # Load x and y from DRAM, masking out any extra elements in case the input is not a
    # multiple of the block size.
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    # Write x + y back to DRAM.
    tl.store(output_ptr + offsets, output, mask=mask)
```

1. `@triton.jit` è£…é¥°å™¨decoratorï¼Œè¡¨ç¤ºä¸‹é¢è¿™æ®µä»£ç æ˜¯ä¸€ä¸ªtriton kernel
2. `x_ptr, y_ptr` æŒ‡é’ˆï¼Œä¸ºå…¶ä»£è¡¨çš„tensorçš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„åœ°å€ã€‚ç”¨æ¥å°†æ•°æ®loadåˆ°memory
3. è¾“å…¥ä¸­ä¸€èˆ¬ä¹Ÿæœ‰strideï¼Œå¯¹äºnç»´çš„tensor aï¼Œa.stride()ä¼šè¾“å‡ºä¸€ä¸ªnç»´æ•°ç»„ã€‚strideç”¨æ¥æ‰¾æ¯ä¸ªå…ƒç´ çš„æŒ‡é’ˆ

```python
a = torch.rand([3,6])
a.stride() # (6, 1)
# è¿™é‡Œçš„ç¬¬ä¸€ä¸ªç»´åº¦çš„ stride æ˜¯ 6, å› ä¸ºä» a[m, k] çš„åœ°å€ åˆ° a[m+1, k] çš„åœ°å€,
```

1. è¶…å‚æ•° `tl.constexptr` ï¼Œå¯¹äºä¸åŒçš„ç¡¬ä»¶ä½¿ç”¨æ—¶ï¼Œæœ€ä½³æ€§èƒ½çš„å‚æ•°å¯èƒ½æ˜¯ä¸åŒçš„ï¼Œåç»­ç”± Triton compiler æ¥è¿›è¡Œæœç´¢ä¸åŒçš„å€¼
2. è™šæ‹Ÿå¾ªç¯ `pid = tl.program_id(axis=0)` ï¼Œæ¯ä¸ªkernelå¯èƒ½è¢«æ‰§è¡Œå¤šæ¬¡
    1. program_idæ˜¯è¿™ä¸ªè™šæ‹Ÿçš„ for "å¾ªç¯" é‡Œé¢çš„ index (ç¬¬å‡ æ¬¡å¾ªç¯ï¼Œå®é™…ä¸­è¿™äº›å¾ªç¯æ˜¯å¹¶è¡Œ)
    2. `axis`Â , æ˜¯è¯´æ˜ "å¾ªç¯"æœ‰å‡ å±‚ï¼Œæ­¤å¤„ axis = 0è¡¨ç¤ºå±•å¼€ä¸º1ç»´æ¥è®¿é—®ï¼ˆç»´åº¦æ¦‚å¿µç±»æ¯”memrefçš„ç»´åº¦ï¼Œç¬¬ä¸€ç»´ç›¸å½“äºmemrefçš„æœ€å†…ç»´uï¼‰
    
    ```python
    pid = tl.program_id(axis=0)
    # å½“è®¿é—®æ•°æ®æ€»é•¿256, BLOCK_SIZE=64
    # tl.arange(0, BLOCK_SIZE) -> [0, 63]
    # 0ï¼Œ 64ï¼Œ 128ï¼Œ 192
    block_start = pid * BLOCK_SIZE
    # æ‰€ä»¥æ•°æ®è®¿é—®æ—¶æ˜¯æŒ‰ç…§ [0:64, 64:128, 128:192, 192:256]
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    ```
    
    > axisæ˜¯å¯åŠ¨3d Gridçš„ç´¢å¼•ï¼Œå¿…é¡»æ˜¯0 / 1 / 2

    c. è°ƒç”¨kernelæ—¶ï¼Œéœ€è¦è¯´æ˜è¯¥kernelæ‰§è¡Œå¾ªç¯æœ‰å‡ å±‚ï¼Œæ¯å±‚æœ‰å‡ æ¬¡ï¼Œè¿™å°±æ˜¯ `grid` çš„æ¦‚å¿µ
    
3. æ˜¾ç¤ºåœ°loadå’Œstoreï¼Œæ‰¹é‡æ•°æ®å¤„ç†ï¼Œä¸€æ¬¡å¤„ç†ä¸€ä¸ªBLOCK_SIZEçš„æ•°æ®ï¼ŒSIMDè¡Œä¸º

```python
    # load å’Œ store æ—¶éƒ½æ˜¯ä½¿ç”¨åŸºåœ°å€åŠ åç§» è·å¾—ä¸€ç‰‡æ•°æ®ï¼Œmaskè¡¨ç¤ºåªè·å¾—è¿™ç‰‡æ•°æ®ä¸­çš„ä¸€éƒ¨åˆ†
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    # å†™å›æ—¶ä¹Ÿéœ€è¦mask
    tl.store(output_ptr + offsets, output, mask=mask)
```


## num_warp

ä¸€èˆ¬ä½“ç°åœ¨module Atträ¸Š

```python
"triton_gpu.num-warps" = 4 : i32
```

tritongpu irç›¸æ¯”ttirä»…å¤šäº†ä¸€ä¸ªBlocked Layoutï¼Œæœ¬è´¨ä¸Šæè¿°çš„æ˜¯Blockå¯¹Memoryçš„Access Pattern

```python
 #blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
```

å°±æ˜¯ä¸€ä¸ªBlocké‡Œæœ‰4ä¸ªWarpï¼Œä¸€ä¸ªWarpæœ‰32ä¸ªThreadï¼Œä¸€ä¸ªThreadå¤„ç†1ä¸ªå…ƒç´ ã€‚

Blocked Layoutåªæ˜¯ä¸€ç§Patternï¼Œä½†æŒ‰ç…§è¿™ä¸ªPatternä¼šå¤šæ¬¡è®¿é—®ï¼Œæ€»è®¿é—®é‡è¾¾åˆ°BLOCK_SIZE

## tl.max_contiguous

```python
  offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
  offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
```

ç”±äºç¼–è¯‘å™¨æ— æ³•æ„ŸçŸ¥æ•°æ®çš„è¿ç»­æ€§ï¼Œæ‰€ä»¥åŠ è½½æ•°æ®æ—¶ä¼š**ç¦»æ•£åœ°**å¤„ç†æ•°æ®ã€‚
å¦‚æœç¼–å†™kernelæ—¶æå‰å·²çŸ¥æ•°æ®è¿ç»­ï¼Œå¯ä»¥ä½¿ç”¨ `tl.max_contiguous & tl.multiple_of` å»æ ‡è¯†åŠ è½½æ•°æ®çš„è¿ç»­æ€§ï¼Œè¿™æ ·ç¼–è¯‘å™¨å°±å¯è¿ç»­åœ°å¤„ç†è¯¥æ®µæ•°æ®ã€‚

```python
  offs_am = tl.max_contiguous(tl.multiple_of((pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M, BLOCK_SIZE_M), BLOCK_SIZE_M)
  offs_am = tl.max_contiguous(tl.multiple_of((pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M, BLOCK_SIZE_M), BLOCK_SIZE_M)
```

## elementwise op