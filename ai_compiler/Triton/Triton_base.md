# triton base

## background

### æ¨èrepo

- ç†è§£tritonè¯­æ³•çš„repoï¼š[triton-puzzles](https://github.com/srush/Triton-Puzzles)

- å¾ˆå¤šç”¨tritonå®ç°çš„kernelçš„repoï¼š[lightllm](https://github.com/ModelTC/lightllm)

### cuda vs triton

cudaå’Œtritonç¼–ç¨‹æ¨¡å¼

<div style="text-align: center;"><img src="./img_Triton_base/cuda_vs_triton.png" alt="cuda_vs_triton" style="width: 90%;"></div>

gpuå±‚æ¬¡ç»“æ„å›¾å¦‚ä¸‹

<div style="text-align: center;"><img src="./img_Triton_base/gpu_arch.png" alt="gpu_arch" style="width: 70%;"></div>

CTAï¼ˆCooperative Thread Arrayï¼‰ï¼šCTAæ˜¯ä¸€ä¸ªçº¿ç¨‹ç»„ï¼Œç”±ä¸€ç»„çº¿ç¨‹ç»„æˆï¼Œè¿™äº›çº¿ç¨‹å¯ä»¥åœ¨GPUä¸Šçš„å¤šä¸ªå¤„ç†å™¨ä¸­å¹¶è¡Œæ‰§è¡Œã€‚**CTAä¸­çš„çº¿ç¨‹å¯ä»¥ååŒå·¥ä½œï¼Œé€šè¿‡å…±äº«å†…å­˜ç­‰æ–¹å¼è¿›è¡Œé€šä¿¡å’Œåä½œ**ã€‚CTAé€šå¸¸æ˜¯åœ¨CUDAç¼–ç¨‹æ¨¡å‹ä¸­ä½¿ç”¨çš„æ¦‚å¿µï¼Œå®ƒæ˜¯å°†å·¥ä½œä»»åŠ¡åˆ’åˆ†ä¸ºè¾ƒå°çš„çº¿ç¨‹å—ä»¥ä¾¿å¹¶è¡Œæ‰§è¡Œçš„åŸºæœ¬å•å…ƒã€‚

**ä¸€ä¸ªCTAé€šå¸¸ç”±å¤šä¸ªwarpç»„æˆ**ã€‚ä¸€ä¸ªCTAçš„çº¿ç¨‹æ•°é‡å¯ä»¥æ˜¯32çš„å€æ•°ï¼ˆä¾‹å¦‚ï¼ŒCTAå¯ä»¥æœ‰32ã€64ã€96ç­‰çº¿ç¨‹ï¼‰ã€‚
CTAå†…çš„çº¿ç¨‹è¢«åˆ’åˆ†ä¸ºä¸€ç»„ä¸€ç»„çš„warpï¼Œæ¯ä¸ªwarpä¸­çš„çº¿ç¨‹åŒæ—¶æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ã€‚

CGAï¼ˆCooperative Grid Arrayï¼‰ï¼šCGAæ˜¯ä¸€ç§æ›´é«˜çº§çš„æ¦‚å¿µï¼Œå®ƒæ˜¯ä¸€ç»„CTAçš„é›†åˆï¼Œå¯ä»¥åœ¨GPUä¸ŠååŒå·¥ä½œã€‚CGAå¯ä»¥ç”¨äºæ›´å¤§è§„æ¨¡çš„å¹¶è¡Œè®¡ç®—ï¼Œå°†ä»»åŠ¡åˆ’åˆ†ä¸ºå¤šä¸ªCTAè¿›è¡Œæ‰§è¡Œï¼Œå¹¶ä¸”CTAä¹‹é—´å¯ä»¥é€šè¿‡å…¨å±€å†…å­˜è¿›è¡Œé€šä¿¡å’ŒåŒæ­¥ã€‚


## elements

è¿™é‡Œåªæ˜¯ç®€å•ä»‹ç»ï¼Œè¯¦ç»†è¯·çœ‹ [Triton_language.md](./Triton_language.md)

ä¸¾ä¸ªğŸŒ°ï¼Œvector add

```python
import torch
import triton
import triton.language as tl

# è¿™æ˜¯kernel
@triton.jit
def add_kernel(x_ptr,  # *Pointer* to first input vector.
               y_ptr,  # *Pointer* to second input vector.
               output_ptr,  # *Pointer* to output vector.
               n_elements,  # Size of the vector.
               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
               # NOTE: `constexpr` so it can be used as a shape value.
               ):
    # There are multiple 'programs' processing different data. We identify which program
    # we are here:
    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.
    # This program will process inputs that are offset from the initial data.
    # For instance, if you had a vector of length 256 and block_size of 64, the programs
    # would each access the elements [0:64, 64:128, 128:192, 192:256].
    # Note that offsets is a list of pointers:
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    # Create a mask to guard memory operations against out-of-bounds accesses.
    mask = offsets < n_elements
    # Load x and y from DRAM, masking out any extra elements in case the input is not a
    # multiple of the block size.
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    # Write x + y back to DRAM.
    tl.store(output_ptr + offsets, output, mask=mask)

# è¿™æ˜¯launchå‡½æ•°
def add(x: torch.Tensor, y: torch.Tensor):
    # We need to preallocate the output.
    output = torch.empty_like(x)
    assert x.is_cuda and y.is_cuda and output.is_cuda
    n_elements = output.numel()
    # The SPMD launch grid denotes the number of kernel instances that run in parallel.
    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].
    # In this case, we use a 1D grid where the size is the number of blocks:
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )
    # NOTE:
    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.
    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.
    #  - Don't forget to pass meta-parameters as keywords arguments.
    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still
    # running asynchronously at this point.
    return output
```

### input

- pointer

`x_ptr, y_ptr` æŒ‡é’ˆï¼Œä¸ºå…¶ä»£è¡¨çš„tensorçš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„åœ°å€ã€‚ç”¨æ¥å°†æ•°æ®loadåˆ°memory

- hyper-parameter

è¶…å‚æ•° `tl.constexptr` ï¼Œè¿è¡Œæ—¶ä¼ å…¥çš„å€¼æ¥è‡ª compiler æœç´¢å¾—åˆ°ã€‚å¯¹äºä¸åŒçš„ç¡¬ä»¶ä½¿ç”¨æ—¶ï¼Œæœ€ä½³æ€§èƒ½ï¼ˆè®¿å­˜+è®¡ç®—ï¼‰çš„å‚æ•°å¯èƒ½æ˜¯ä¸åŒçš„ï¼Œåç»­ç”± Triton compiler æ¥è¿›è¡Œæœç´¢ä¸åŒçš„å€¼

- stride

è¾“å…¥ä¸­ä¸€èˆ¬ä¹Ÿæœ‰strideï¼Œå¯¹äºnç»´çš„tensor aï¼Œa.stride()ä¼šè¾“å‡ºä¸€ä¸ªnç»´æ•°ç»„ã€‚strideç”¨æ¥æ‰¾æ¯ä¸ªå…ƒç´ çš„æŒ‡é’ˆ

### pid

è™šæ‹Ÿå¾ªç¯ `pid = tl.program_id(axis=0)` ï¼Œæ¯ä¸ªkernelå¯èƒ½è¢«æ‰§è¡Œå¤šæ¬¡

1. program_idæ˜¯è¿™ä¸ªè™šæ‹Ÿçš„**for å¾ªç¯ é‡Œé¢çš„ index** (ç¬¬å‡ æ¬¡å¾ªç¯ï¼Œå®é™…ä¸­è¿™äº›å¾ªç¯æ˜¯å¹¶è¡Œ)

```python
pid = tl.program_id(axis=0)
# å½“è®¿é—®æ•°æ®æ€»é•¿256, BLOCK_SIZE=64
# tl.arange(0, BLOCK_SIZE) -> [0, 63]
# 0ï¼Œ 64ï¼Œ 128ï¼Œ 192
block_start = pid * BLOCK_SIZE
# æ‰€ä»¥æ•°æ®è®¿é—®æ—¶æ˜¯æŒ‰ç…§ [0:64, 64:128, 128:192, 192:256]
offsets = block_start + tl.arange(0, BLOCK_SIZE)
```

2. `axis`Â , æ˜¯è¯´æ˜ "å¾ªç¯"æœ‰å‡ å±‚ï¼Œæ­¤å¤„ axis = 0è¡¨ç¤ºå±•å¼€ä¸º1ç»´æ¥è®¿é—®ï¼ˆç»´åº¦æ¦‚å¿µç±»æ¯”memrefçš„ç»´åº¦ï¼Œç¬¬ä¸€ç»´ç›¸å½“äºmemrefçš„æœ€å†…ç»´uï¼‰

axisæ˜¯å¯åŠ¨3d Gridçš„ç´¢å¼•ï¼Œå¿…é¡»æ˜¯0 / 1 / 2

### load & store

æ˜¾ç¤ºåœ°loadå’Œstoreï¼Œæ‰¹é‡æ•°æ®å¤„ç†ï¼Œä»¥BLOCKä¸ºå•ä½ï¼Œä¸€æ¬¡å¤„ç†ä¸€ä¸ªBLOCK_SIZEçš„æ•°æ®ï¼ŒSIMDè¡Œä¸º

loadï¼šä»DRAMè¯»åˆ°SRAMï¼›storeï¼šä»SRAMå†™å›DRAMï¼›å‡å°‘äº†DRAMä¸Šè®¡ç®—è¡Œä¸ºã€‚loadå’Œstoreåœ¨è¯­ä¹‰ä¸Šå¯ä»¥è§£é‡Šä¸ºgatherå’Œscatter

```python
# load å’Œ store æ—¶éƒ½æ˜¯ä½¿ç”¨åŸºåœ°å€åŠ åç§» è·å¾—ä¸€ç‰‡æ•°æ®ï¼Œmaskè¡¨ç¤ºåªè·å¾—è¿™ç‰‡æ•°æ®ä¸­çš„ä¸€éƒ¨åˆ†
x = tl.load(x_ptr + offsets, mask=mask)
y = tl.load(y_ptr + offsets, mask=mask)
# å†™å›æ—¶ä¹Ÿéœ€è¦mask
tl.store(output_ptr + offsets, output, mask=mask)
```

- offsets

offsets ä¸ºè¦å¤„ç†æ•°æ®çš„èŒƒå›´ï¼Œç”±å½“å‰block_startå’Œrangeè®¡ç®—è€Œæˆ

```python
block_start = pid * BLOCK_SIZE
offsets = block_start + tl.arange(0, BLOCK_SIZE)
```

- mask

mask ä¸ºé®ç›–ï¼Œç±»ä¼¼decoder Attnä¸­çš„maskã€‚ä¸€æ˜¯è§„èŒƒè®¿å­˜è¡Œä¸ºï¼Œé˜²æ­¢è¶Šç•Œï¼ˆæœ€åä¸€å—æ•°æ®å¤§å°å¯èƒ½ä¸æ»¡è¶³é˜²æ­¢ BLOCK_SIZE çš„å¤§å°ï¼‰ï¼›äºŒæ˜¯è¿‡æ»¤å¯¹æœ¬æ¬¡è®¡ç®—ä¸å¿…é¡»çš„æ•°æ®ã€‚

ä¾‹å¦‚offset=1024ï¼Œmaskä¸ºä¸€ä¸ª1024ç»´çš„æ•°ç»„ï¼Œæ¯ä¸ªæ•°ä¸º0/1ï¼Œå½“æŸä½ä¸º1æ—¶ï¼Œåˆ™loadè¯¥æ•°æ®ï¼Œå½“æŸä½ä¸º0æ—¶ï¼Œèˆå¼ƒã€‚

### detector

- @triton.jitï¼šè¡¨ç¤ºä¸‹é¢è¿™æ®µä»£ç æ˜¯ä¸€ä¸ªtriton kernel

- @[auto-tuning](https://triton-lang.org/main/python-api/generated/triton.autotune.html) ï¼šç”± `@triton.jit`è£…é¥°çš„kernelå¯ä»¥è°ƒç”¨ `@auto-tuning` detectorè§¦å‘è‡ªåŠ¨è°ƒä¼˜

ä½¿ç”¨ä¸Šéœ€è¦æä¾›ä¸€ä¸ªconfigsï¼ˆåŒ…å«åœ¨kernelä¸­å®šä¹‰çš„ `tl.constexpr`ï¼‰åˆ—è¡¨ï¼Œautotuneä¼šå¤šæ¬¡è¿è¡Œkernelå‡½æ•°æ¥è¯„ä¼°configsä¸­çš„æ‰€æœ‰é…ç½®ã€‚ï¼ˆé…ç½®æ˜¯äººä¸ºç»™å‡ºçš„ï¼Œæ‰€ä»¥ç©ºé—´ä¸å¤§ï¼Œä¾èµ–äººä¸ºç»éªŒï¼‰

- keyï¼šå‚æ•°åˆ—è¡¨ï¼Œå½“keyä¸­çš„å‚æ•°æ”¹å˜æ—¶ï¼Œéœ€è¦é‡æ–°è¯„ä¼°configs

- prune_configs_byï¼šç”¨æˆ·å¯ä»¥ä¼ å…¥å‡½æ•°æ¥å¸®åŠ©å‡æï¼ˆä¾‹å¦‚åŸºäºæ€§èƒ½æ¨¡å‹çš„å‡½æ•°ï¼‰ï¼ŒåŠ å¿«æ”¶æ•›

- reset_to_zeroï¼šè¾“å…¥å‚æ•°ååˆ—è¡¨ï¼Œåœ¨è¿è¡Œå‰å°†è¿™äº›å‚æ•°é‡ç½®ä¸º0

- warmupï¼šæ¯ä¸ªconfigçš„warmupæ—¶é—´ï¼Œé»˜è®¤25ms

- repï¼šæ¯ä¸ªconfigçš„é‡å¤æ—¶é—´ï¼Œé»˜è®¤100ns

```python
@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
    ],
    key=['M', 'N', 'K'],
)
```

### grid

è°ƒç”¨kernelæ—¶ï¼Œéœ€è¦è¯´æ˜è¯¥kernelæ‰§è¡Œå¾ªç¯æœ‰å‡ å±‚ï¼Œæ¯å±‚æœ‰å‡ æ¬¡ï¼Œè¿™å°±æ˜¯ `grid` çš„æ¦‚å¿µ

ä¸‹è¿°ä»£ç è¡¨ç¤ºäº†è¿™ä¸ª vector-add kernelæ˜¯åœ¨ä¸€å±‚forå¾ªç¯å†…è°ƒç”¨æ‰§è¡Œï¼Œæ¯æ¬¡æ•°æ®å¤§å° `BLOCK_SIZE` 

```python
  grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )
```

Tritonä¸­å…³äºgridå®šä¹‰ï¼š

```python
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
    )
    matmul_kernel[grid](
        a, b, c,
        M, N, K,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
        ACTIVATION=activation
    )
```

å¯¹æ¯”Cudaä¸­launch kernelçš„è¡Œä¸º

```cpp
  dim3 block(BLOCK_SIZE_M, BLOCK_SIZE_N);  
  dim3 grid((M + BLOCK_SIZE_M - 1) / BLOCK_SIZE_M, (N + BLOCK_SIZE_N - 1) / BLOCK_SIZE_N);
  matmul_kernel<<<grid,block>>>(Ad, Bd, Cd, M, N, K);
```

## special

### SIMDçš„ç¼–ç¨‹èŒƒå¼

æ¯”CUDAçš„SIMTç¼–ç¨‹èŒƒå¼ï¼Œç”±å¤šä¸ªthreadå¹¶è¡Œå¤„ç†

tritonæ˜¯SIMDç¼–ç¨‹èŒƒå¼ï¼Œä¸€æ¬¡å¤„ç†ä¸€ç‰‡æ•°æ®ï¼ˆåŸºäºblockç®—æ³•çš„ç¼–ç¨‹èŒƒå¼ï¼‰

ç›´æ¥å¯¹çº¿ç¨‹å—è¿›è¡Œç¼–ç¨‹ï¼Œæ¯ä¸€ä¸ªæ“ä½œéƒ½æ˜¯åº”ç”¨åœ¨å—ä¸Šï¼Œä¸å†æ§åˆ¶å•ä¸ªçš„çº¿ç¨‹ï¼Œçœå»çº¿ç¨‹ä¹‹é—´çš„åŒæ­¥ç­‰æ“ä½œ

<div style="text-align: center;"><img src="./img_Triton_base/cuda_triton.png" alt="cuda_triton" style="width: 90%;"></div>


### block-level control- and data-flow analysis

triton compilerä¾èµ–block-level control- and data-flow analysisæ¥é™æ€åœ°schedule iterator blocks

ç¦»æ•£ä¼˜åŒ–ï¼šå°½é‡ä¿è¯æ•°æ®åŠ è½½è¿ç»­æ€§â€”>åˆ†ææ¯ä¸€æ­¥æ“ä½œå¹¶æ€»ç»“å‡ºstrideå’ŒstrideValï¼Œæœ€ç»ˆç”¨äºé™æ€ä¿¡æ¯å°†tl.loadä¼˜åŒ–æˆtensor.extract_sliceï¼ˆä¸‹é™ç»“æœirä¸­æœ€è€—æ—¶çš„æ˜¯copyï¼‰ï¼Œæ¯”d2dçš„ç¦»æ•£è®¿å­˜é€Ÿåº¦å¿«

### grid æ¯ä¸ªtriton kernelè·‘åœ¨ä¸€ä¸ªgridå†…

è°ƒç”¨kernelæ—¶ï¼Œéœ€è¦è¯´æ˜è¯¥kernelæ‰§è¡Œå¾ªç¯æœ‰å‡ å±‚ï¼Œæ¯å±‚æœ‰å‡ æ¬¡ï¼Œè¿™å°±æ˜¯ `grid` çš„æ¦‚å¿µ

ä»¥Matmulè€Œè¨€ï¼Œè‹¥Aä¸ºMxKï¼ŒBä¸ºKxNï¼Œé‚£ä¹ˆCçš„å¤§å°å°±æ˜¯MxNï¼ˆMå’ŒNä¸ºparallel axiså¤§å°ï¼ŒKä¸ºreductionè½´å¤§å°ï¼‰

æ¯æ¬¡åˆ†å—è®¡ç®—ï¼Œå•å—å¤§å°BLOCK_SIZE_M x BLOCK_SIZE_Nï¼Œæ€»å…±è¿›è¡Œ 
$$
\frac{M}{\text{BLOCK\_{SIZE}\_{M}}} \times \frac{N}{\text{BLOCK\_{SIZE}\_{N}}}
$$
Tritonä¸­å…³äºgridå®šä¹‰ï¼š

```python
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
    )
    matmul_kernel[grid](
        a, b, c,
        M, N, K,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
        ACTIVATION=activation
    )
```

å¯¹æ¯”Cudaä¸­launch kernelçš„è¡Œä¸º

```cpp
  dim3 block(BLOCK_SIZE_M, BLOCK_SIZE_N);  
  dim3 grid((M + BLOCK_SIZE_M - 1) / BLOCK_SIZE_M, (N + BLOCK_SIZE_N - 1) / BLOCK_SIZE_N);
  matmul_kernel<<<grid,block>>>(Ad, Bd, Cd, M, N, K);
```

ä¸‹é¢çš„group-orderçš„è¡Œä¸ºèƒ½è·å¾—æ›´å¥½çš„data-reuse

<div style="text-align: center;"><img src="./img_Triton_base/layout.png" alt="layout" style="width: 90%;"></div>

åˆ†æï¼šAå’ŒBä¸­çš„å†…å®¹éƒ½æ˜¯è¡Œä¼˜å…ˆå­˜å‚¨ï¼Œä»¥è®¡ç®—ä¹ä¸ªæ•°ä¸ºä¾‹ï¼Œé‚£ä¹ˆåŸå§‹çš„ä¸€æ¬¡loadéœ€è¦9+9$\times$9=90æ¬¡readå’Œ9æ¬¡writeã€‚è€Œgroup orderä¸­ï¼Œä¸€æ¬¡loadéœ€è¦9$\times$3+3$\times$9=54æ¬¡readå’Œ9æ¬¡write

- num_pid_m å’Œ num_pid_n å°±æ˜¯ä¸ºæ¥è·å¾—çŸ©é˜µé•¿å®½å„å¯ä»¥åˆ†ä¸ºå¤šå°‘ä¸ªblockï¼ˆä¸Šå›¾çš„é»„è‰²å°å—ï¼‰

```python
pid = tl.program_id(axis=0)
# number of program ids along the M / N axis 
num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
```

- num_pid_in_group  è¡¨ç¤ºä¸€ä¸ªé«˜æ˜¯Â `GROUP_SIZE_M`Â , å®½æ˜¯Â `num_pid_n`çš„groupä¸­åŒ…å«å¤šå°‘ä¸ªé»„è‰²å°å—

```python
# number of program in group
num_pid_in_group = GROUP_SIZE_M * num_pid_n
```

- group_idè¡¨ç¤ºå½“å‰å¾ªç¯iteræ˜¯åœ¨å“ªä¸ªgroupå†…

```python
# id of the group which related to this program
group_id = pid // num_pid_in_group
```

- first_pid_m è¡¨ç¤ºå½“å‰æ‰€åœ¨çš„çš„groupå†…çš„ç¬¬ä¸€ä¸ªé»„è‰²blockæ˜¯å…¨å±€çš„ç¬¬å‡ ä¸ªé»„è‰²blockï¼ˆä»mçš„ç»´åº¦ä¸Šçœ‹ï¼‰

```python
# row-id of the first program in the group
first_pid_m = group_id * GROUP_SIZE_M = (pid // (GROUP_SIZE_M * num_pid_n)) * GROUP_SIZE_M
```

- é‡å¤è®¡ç®—ä¸‹group_size_mï¼Œé˜²æ­¢è¶Šç•Œ

```python
group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
```

- å¾—åˆ°å½“å‰å¾ªç¯éœ€è¦å¤„ç†å“ªä¸ªå— [pid_m, pid_n]

pid_m â‰¤ first_pid_m + group_size_m

pid_n æ˜¯ä»å·¦åˆ°å³ä¸€åˆ—åˆ—æ¥çš„ï¼Œ000111222

```python
# row-id of the p in the launch grid
pid_m = first_pid_m + pid % group_size_m # è¡Œid
# col-id of the p in the launch grid
pid_n = (pid % num_pid_in_group) // group_size_m # åˆ—id
# num_pid_in_group = GROUP_SIZE_M * num_pid_n
```

a_ptr æ˜¯AçŸ©é˜µç¬¬ä¸€ä¸ªå…ƒç´ çš„åœ°å€

`offs_am`Â å’ŒÂ `offs_bn`Â æ˜¯ A çŸ©é˜µ 9 ä¸ª block ä¸­ç¬¬ä¸€ä¸ª block ä¸­, æ¯ä¸ªå…ƒç´ åœ¨æ•´ä¸ª A çŸ©é˜µä¸­çš„åæ ‡ï¼Œå³ m ç»´åº¦çš„ index å’Œ k ç»´åº¦çš„ index

```python
    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers
    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)
```

```python
offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
c_ptrs = c+ptr + stride_cm * offset_cm[:, None] + stride_cn * offset_cn[None, :]
c_mask = (offset_cm[:, None] < M) & (offset_cn[None, :] < N)
tl.store(c_ptrs, mask=c_mask)
```

è®¡ç®—å¾ªç¯ï¼Œmaskä¿è¯loadå’Œstoreä¸è¶Šç•Œ

```python
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        # Load the next block of A and B, generate a mask by checking the K dimension.
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        # We accumulate along the K dimension.
        accumulator += tl.dot(a, b)
        # è®¡ç®—ä¸‹Kä¸ªBLOCK
        a_ptrs += BLOCK_SIZE_K * stride_ak
        b_ptrs += BLOCK_SIZE_K * stride_bk
```


### num_warp

ä¸€èˆ¬ä½“ç°åœ¨module Atträ¸Šï¼Œä¸‹é¢çš„ä»£ç æ„å‘³ç€è¿™ä¸ªç¨‹åºä½¿ç”¨4ä¸ªwarpæ‰§è¡Œï¼ˆè¿™ä¸ªå‚æ•°ä¸€èˆ¬ä¹Ÿæ˜¯ `tl.constexpr`ï¼‰

```python
"triton_gpu.num-warps" = 4 : i32
```

tritongpu irç›¸æ¯”ttirä»…å¤šäº†ä¸€ä¸ªBlocked Layoutï¼Œæœ¬è´¨ä¸Šæè¿°çš„æ˜¯Blockå¯¹Memoryçš„Access Pattern

```python
 #blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
```

å°±æ˜¯ä¸€ä¸ªCTAé‡Œæœ‰4ä¸ªWarpï¼Œä¸€ä¸ªWarpæœ‰32ä¸ªThreadï¼Œä¸€ä¸ªThreadå¤„ç†1ä¸ªå…ƒç´ ã€‚

Blocked Layoutåªæ˜¯ä¸€ç§Patternï¼Œä½†æŒ‰ç…§è¿™ä¸ªPatternä¼šå¤šæ¬¡è®¿é—®ï¼Œæ€»è®¿é—®é‡è¾¾åˆ°BLOCK_SIZE

### layout

Layoutï¼šå®šä¹‰äº†Dataæ˜¯å¦‚ä½•è¢«Threadå¤„ç†

- **Distributed Layoutï¼š**Blocked Layout, MMA Layout, DotOperand Layoutéƒ½å±äºæ­¤ç±»ã€‚è¿™äº›Layoutçš„ç‰¹ç‚¹éƒ½æ˜¯æ˜ å°„å‡½æ•°ä¼šå°†ç‰¹å®šçš„Tensoräº¤ç»™ç‰¹å®šçš„Threadå»å¤„ç†ï¼Œè¾¾åˆ°ä¸€ä¸ª**distribution**çš„æ•ˆæœ
- **Shared Layoutï¼š**GPUä¸­çš„Shared Memoryæ˜¯å¯ä»¥è¢«ä¸€ä¸ªBlockå†…çš„ä»»æ„çº¿ç¨‹è®¿é—®çš„ï¼Œæ˜ å°„å‡½æ•°è¢«å®šä¹‰ä¸ºä»»æ„Tensor->ä»»æ„Thread

#### distributed layout

Distributed encodings have a layout function that is entirely characterized by a d-dimensional tensor L. Note that L doesn't need to have the same shape (or even the same rank) as the tensor it is encoding.

<div style="text-align: center;"><img src="./img_Triton_base/distribute_layout.png" alt="distribute_layout" style="width: 90%;"></div>

#### block layout

An encoding where each warp owns a contiguous portion of the target tensor. This is typically the kind of data layout **used to promote memory coalescing in LoadInst and StoreInst.**

`#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>`

<img src="./img_Triton_base/cta_wrap_thread.png" alt="Untitled" style="zoom:50%;" />

- **sizePerThread = [1, 8]ï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç†æ•°æ®Size**
- **threadsPerWarp = [8, 4]ï¼š warpå†…çº¿ç¨‹çš„å¸ƒå±€**
- **warpsPerCTA = [8, 1]ï¼šCTAï¼ˆBlockï¼‰å†…warpçš„å¸ƒå±€**
- **order = [1, 0]ï¼šæŒ‰è¡Œè®¿é—®**

è¯¥BLockè®¿å­˜æ¨¡å¼ä¸€æ¬¡èƒ½å¤„ç†(1x8x8, 8x4) = (64, 32)è§„æ¨¡çš„shapeã€‚ä½†è‹¥è¾“å…¥opçš„shapeä¸º(128, 32)ï¼Œé‚£ä¹ˆè®©æ¯ä¸ªthreadå¤„ç†ä¸¤ä¸ªè¿ç»­å—å³å¯ï¼Œå³ç¬¬ä¸€ä¸ªthreadå¤„ç†(0, 0:7), (64, 0:7)ä¸¤ä¸ªå—

#### shared layout

In order to **avoid shared memoryÂ bank conflicts**, elements may beÂ **swizzled**Â in memory. 

åŒä¸€ä¸ªwarpå†…çš„threadåŒæ—¶è®¿é—®åŒä¸€åˆ—çš„æ•°æ®

<div style="text-align: center;"><img src="./img_Triton_base/swizzled.png" alt="swizzled memory" style="width: 90%;"></div>

### triton compiler

<div style="text-align: center;"><img src="./img_Triton_base/triton_arch_now.png" alt="triton_arch_now" style="width: 70%;"></div>

compileræ”¯æŒå¤šåç«¯çš„æ–¹å‘ï¼šé€šè¿‡Linalg dialect

<div style="text-align: center;"><img src="./img_Triton_base/triton_arch.png" alt="triton_arch" style="width: 70%;"></div>


## trick

### æ‰“å°irçš„æ–¹æ³•

åœ¨kernelåå¢åŠ 

```python
     kernel = add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
     print(kernel.asm['ttir'])
     print(kernel.asm['ttgir'])
     print(kernel.asm['llir'])
     print(kernel.asm['ptx'])
```
